\chapter{Experiments}



\section{Dataset}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/fine_grained_distribution.png}
\caption{\textbf{LOGIC Dataset}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/copi_distribution.png}
\caption{\textbf{LOGIC Dataset to coarse-grained classes by Copi}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/aristotle_distribution.png}
\caption{\textbf{LOGIC Dataset to coarse-grained classes by Aristotle}}
\end{figure}

\section{Zero-shot}


\subsection{ Fine Grained Logic Classes}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown labels \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 21.000 & 0.081 & 2.333 & 7.667 \\
falcon-mamba-7b-instruct & True & 1 & 20.333 & 0.086 & 2.667 & 11.667 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 40.333 & 0.211 & 0.000 & 2.333 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 38.333 & 0.208 & 0.000 & 1.667 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 39.333 & 0.153 & 0.000 & 6.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 34.667 & 0.210 & 0.000 & 7.333 \\
falcon-mamba-7b-instruct & False & 2 & 24.667 & 0.095 & 6.333 & 9.000 \\
falcon-mamba-7b-instruct & True & 2 & 22.667 & 0.082 & 6.000 & 13.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 40.000 & 0.216 & 0.000 & 2.333 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & \textbf{41.667} & \textbf{0.237} & 0.000 & 1.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 39.333 & 0.158 & 0.000 & 4.667 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 33.000 & 0.189 & 0.000 & 8.333 \\
\bottomrule
\end{tabular}}
\caption{Fine-grained Classes Results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/conf_matrix_meta-llama-Meta-Llama-3.1-8B-Instruct_prompt2_no_cot_FINE_GRAINED_results_with_definitions_42_no_sampling.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}


\subsection{ Coarse Grained Copi Classes}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 43.667 & 0.080 & 2.000 & 2.667 \\
falcon-mamba-7b-instruct & True & 1 & 51.000 & 0.134 & 5.000 & 1.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 55.333 & 0.371 & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & \textbf{61.000} & \textbf{0.408} & 0.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 44.333 & 0.102 & 0.000 & 4.667 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.333 & 0.083 & 0.000 & 2.667 \\
falcon-mamba-7b-instruct & False & 2 & 30.333 & 0.036 & 0.333 & 9.333 \\
falcon-mamba-7b-instruct & True & 2 & 38.667 & 0.056 & 1.000 & 9.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 59.000 & 0.327 & 0.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 54.333 & 0.312 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 33.333 & 0.060 & 0.000 & 11.667 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 47.667 & 0.069 & 0.000 & 12.333 \\
\bottomrule
\end{tabular}}
\caption{Coarse-grained Copi Classes Results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/conf_matrix_meta-llama-Meta-Llama-3.1-8B-Instruct_prompt1_no_cot_COPI_results_with_definitions_42_no_sampling.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 54.000 & 0.276 & 10.000 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 50.000 & 0.260 & 14.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 67.000 & 0.389 & 2.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 64.000 & 0.380 & 1.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 61.000 & 0.332 & 6.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 62.333 & 0.359 & 7.333 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 52.333 & 0.280 & 15.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 48.667 & 0.272 & 19.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 64.667 & 0.396 & 2.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & \textbf{67.000} & \textbf{0.411} & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 64.000 & 0.397 & 4.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 60.000 & 0.354 & 8.333 & 0.000 \\
\bottomrule
\end{tabular}}
\caption{Results when mapping fine-grained results to Copi's coarse-grained classes}
\end{table}

\subsection{ Coarse Grained Aritotle Classes}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 36.333 & 0.127 & 1.333 & 1.667 \\
falcon-mamba-7b-instruct & True & 1 & 34.333 & 0.072 & 1.667 & 5.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & \textbf{51.667} & \textbf{0.465} & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 55.667 & 0.303 & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 52.333 & 0.116 & 0.000 & 8.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.667 & 0.116 & 0.000 & 8.333 \\
falcon-mamba-7b-instruct & False & 2 & 41.333 & 0.136 & 0.333 & 2.333 \\
falcon-mamba-7b-instruct & True & 2 & 41.333 & 0.104 & 0.000 & 4.333 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 41.000 & 0.272 & 0.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 54.333 & 0.292 & 0.333 & 0.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 56.333 & 0.094 & 0.000 & 10.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 59.667 & 0.285 & 0.000 & 1.667 \\
\bottomrule
\end{tabular}}
\caption{Coarse-grained Aritotle Classes Results}
\end{table}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/conf_matrix_meta-llama-Meta-Llama-3.1-8B-Instruct_prompt1_no_cot_ARISTOTLE_results_no_definitions_42_no_sampling.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 41.333 & 0.328 & 10.000 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 39.000 & 0.303 & 14.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 60.333 & 0.422 & 2.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 60.333 & 0.435 & 1.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 57.667 & 0.456 & 6.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 53.667 & 0.422 & 7.333 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 41.667 & 0.352 & 15.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 40.000 & 0.329 & 19.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{63.333} & 0.446 & 2.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 62.333 & \textbf{0.465} & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 59.333 & 0.402 & 4.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 52.333 & 0.360 & 8.333 & 0.000 \\
\bottomrule
\end{tabular}}
\caption{Results when mapping fine-grained results to Aritotle's coarse-grained classes}
\end{table}









\section{Basic CoT}

\subsection{ Fine Grained Logic Classes}
\begin{table}[H]
\centering
\caption{Fine-grained Classes Results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 17.667 & 0.062 & 5.000 & 9.000 \\
falcon-mamba-7b-instruct & True & 1 & 18.333 & 0.081 & 0.333 & 5.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 39.333 & 0.213 & 0.000 & 3.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 36.667 & 0.251 & 0.000 & 0.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 39.667 & 0.161 & 0.000 & 5.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 34.333 & 0.198 & 0.000 & 7.333 \\
falcon-mamba-7b-instruct & False & 2 & 22.667 & 0.087 & 2.333 & 10.000 \\
falcon-mamba-7b-instruct & True & 2 & 23.000 & 0.081 & 3.000 & 15.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 41.000 & 0.230 & 0.000 & 3.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & \textbf{42.000} & \textbf{0.266} & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 37.667 & 0.171 & 0.000 & 5.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 32.333 & 0.162 & 0.000 & 7.333 \\
\bottomrule
\end{tabular}}
\caption{Fine-grained Classes Results}
\end{table}



\subsection{ Coarse Grained Copi Classes}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 49.333 & 0.102 & 2.667 & 3.000 \\
falcon-mamba-7b-instruct & True & 1 & 53.333 & 0.132 & 1.000 & 0.667 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 59.667 & \textbf{0.405} & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & \textbf{61.000} & 0.403 & 0.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 38.667 & 0.089 & 0.000 & 3.667 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.667 & 0.102 & 0.000 & 1.667 \\
falcon-mamba-7b-instruct & False & 2 & 34.667 & 0.071 & 2.667 & 5.667 \\
falcon-mamba-7b-instruct & True & 2 & 38.333 & 0.070 & 0.000 & 12.333 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 58.000 & 0.388 & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 51.333 & 0.284 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 30.667 & 0.051 & 0.000 & 12.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 43.667 & 0.064 & 0.000 & 13.333 \\
\bottomrule
\end{tabular}}
\caption{Coarse-grained Copi Classes Results}
\end{table}



\subsection{ Coarse Grained Aritotle Classes}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 27.333 & 0.078 & 3.333 & 3.333 \\
falcon-mamba-7b-instruct & True & 1 & 29.667 & 0.079 & 1.000 & 2.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 52.667 & \textbf{0.437} & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & \textbf{58.000} & 0.379 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 55.667 & 0.158 & 0.000 & 4.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 55.000 & 0.175 & 0.000 & 4.667 \\
falcon-mamba-7b-instruct & False & 2 & 38.000 & 0.137 & 1.000 & 1.333 \\
falcon-mamba-7b-instruct & True & 2 & 39.333 & 0.085 & 0.667 & 3.333 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 39.000 & 0.253 & 0.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 53.667 & 0.377 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 54.000 & 0.091 & 0.000 & 10.333 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 59.000 & 0.190 & 0.000 & 3.000 \\
\bottomrule
\end{tabular}}
\caption{Coarse-grained Aritotle Classes Results}
\end{table}


\begin{table} [H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 46.333 & 0.199 & 14.000 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 49.333 & 0.221 & 5.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 65.667 & 0.414 & 3.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 60.000 & 0.367 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 63.333 & 0.358 & 5.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 63.000 & 0.362 & 7.333 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 51.000 & 0.266 & 12.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 48.333 & 0.286 & 18.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{67.667} & \textbf{0.428} & 3.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 66.667 & 0.398 & 0.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 64.333 & 0.430 & 5.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 62.333 & 0.341 & 7.333 & 0.000 \\
\bottomrule
\end{tabular}}
\caption{COPI Coarse-Grained to Fine-Grained Basic CoT}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 37.667 & 0.316 & 14.000 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 45.667 & 0.342 & 5.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 59.667 & 0.398 & 3.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 60.667 & 0.427 & 0.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 58.333 & 0.459 & 5.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 53.333 & 0.420 & 7.333 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 36.667 & 0.312 & 12.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 38.667 & 0.317 & 18.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{63.333} & \textbf{0.465} & 3.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 61.667 & 0.425 & 0.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 56.667 & 0.372 & 5.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 50.333 & 0.331 & 7.333 & 0.000 \\
\bottomrule
\end{tabular}}
\caption{ARISTOTLE Coarse-Grained to Fine-Grained Basic CoT}
\end{table}





\section{Multi-Round CoT (From Coarse-Grained to Fine-Grained)}




\begin{table} [H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllllll}
\toprule
model & def & prompt & r1\_accuracy & r1\_f1 & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 35.333 & 0.053 & 5.000 & 0.032 & 0.333 & 60.000 \\
falcon-mamba-7b-instruct & True & 1 & 47.000 & 0.082 & 7.000 & 0.025 & 0.333 & 53.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 55.000 & 0.376 & 32.667 & 0.213 & 0.000 & 1.333 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 57.333 & 0.379 & 31.667 & 0.212 & 0.000 & 1.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 39.333 & 0.051 & 30.333 & 0.179 & 0.000 & 1.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 49.667 & 0.057 & 28.667 & 0.176 & 0.000 & 1.333 \\
falcon-mamba-7b-instruct & False & 2 & 25.333 & 0.031 & 8.333 & 0.045 & 0.000 & 50.333 \\
falcon-mamba-7b-instruct & True & 2 & 38.333 & 0.047 & 11.333 & 0.051 & 0.000 & 29.333 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 56.333 & 0.390 & 32.000 & 0.231 & 0.000 & 0.333 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 50.000 & 0.359 & \textbf{32.667} & \textbf{0.247} & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 32.667 & 0.042 & 31.000 & 0.157 & 0.000 & 1.667 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 44.667 & 0.053 & 34.333 & 0.160 & 0.000 & 4.333 \\
\bottomrule
\end{tabular}
}
\caption{COPI Coarse-Grained to Fine-Grained Multi-round CoT}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrllllll}
\toprule
model & def & prompt & r1\_accuracy & r1\_f1 & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 42.000 & 0.139 & 7.333 & 0.035 & 0.333 & 31.000 \\
falcon-mamba-7b-instruct & True & 1 & 39.000 & 0.055 & 10.333 & 0.054 & 0.333 & 20.667 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 56.000 & 0.470 & 32.333 & 0.215 & 0.000 & 1.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 55.667 & 0.500 & 32.000 & 0.233 & 0.000 & 3.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 43.667 & 0.032 & 35.333 & 0.128 & 0.000 & 5.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.000 & 0.048 & 26.667 & 0.134 & 0.000 & 7.000 \\
falcon-mamba-7b-instruct & False & 2 & 43.000 & 0.099 & 7.667 & 0.043 & 0.000 & 39.333 \\
falcon-mamba-7b-instruct & True & 2 & 38.667 & 0.076 & 11.667 & 0.069 & 0.000 & 18.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 44.667 & 0.411 & 30.333 & 0.196 & 0.000 & 1.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 56.333 & 0.382 & 35.333 & \textbf{0.250} & 0.000 & 4.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 43.333 & 0.040 & \textbf{35.667} & 0.188 & 0.000 & 3.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 52.000 & 0.096 & 31.333 & 0.175 & 0.000 & 11.333 \\
\bottomrule
\end{tabular}
}
\caption{ARISTOTLE Coarse-Grained to Fine-Grained Multi-round CoT}
\end{table}