\chapter{Experiments}



\section{Dataset}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/fine_grained_distribution.png}
\caption{\textbf{LOGIC Dataset}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/copi_distribution.png}
\caption{\textbf{LOGIC Dataset to coarse-grained classes by Copi}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/aristotle_distribution.png}
\caption{\textbf{LOGIC Dataset to coarse-grained classes by Aristotle}}
\end{figure}

\section{Zero-shot}


\subsection{ Fine Grained Logic Classes}
\begin{table}[H]
\centering
\caption{Fine-grained Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown labels \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 19.667 & 0.062 & 6.667 & 9.333 \\
falcon-mamba-7b-instruct & True & 1 & 20.333 & 0.071 & 5.667 & 12.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 41.333 & 0.219 & 0.000 & 1.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 39.000 & 0.242 & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 39.000 & 0.133 & 0.000 & 4.667 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 34.667 & 0.186 & 0.000 & 6.333 \\
falcon-mamba-7b-instruct & False & 2 & 20.000 & 0.060 & 5.333 & 24.000 \\
falcon-mamba-7b-instruct & True & 2 & 19.667 & 0.063 & 14.667 & 21.667 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{41.000} & 0.191 & 0.333 & 1.667 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 40.667 & \textbf{0.219} & 0.333 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 38.667 & 0.161 & 0.000 & 3.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 31.333 & 0.148 & 0.000 & 9.000 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/fine_grained_best_confusion_matrix.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}


\subsection{ Coarse Grained Copi Classes}
\begin{table}[H]
\centering
\caption{Coarse-grained Copi Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 41.333 & 0.082 & 8.000 & 3.000 \\
falcon-mamba-7b-instruct & True & 1 & 45.667 & 0.102 & 11.667 & 2.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 49.333 & 0.277 & 0.000 & 0.333 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & \textbf{57.000} & 0.259 & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 43.333 & 0.086 & 0.000 & 5.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.333 & 0.084 & 0.000 & 2.333 \\
falcon-mamba-7b-instruct & False & 2 & 29.667 & 0.036 & 0.333 & 21.333 \\
falcon-mamba-7b-instruct & True & 2 & 14.000 & 0.027 & 0.333 & 64.333 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 55.000 & \textbf{0.300} & 0.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 51.667 & 0.231 & 0.333 & 0.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 32.000 & 0.056 & 0.000 & 12.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 48.000 & 0.071 & 0.000 & 12.333 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/copi_confusion_matrix.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}

\begin{table}[H]
\centering
\caption{Results when mapping fine-grained results to Copi's coarse-grained classes}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 50.000 & 0.263 & 16.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 47.667 & 0.251 & 18.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 70.000 & 0.377 & 1.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & \textbf{67.333} & 0.366 & 0.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 62.333 & 0.301 & 6.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 63.000 & 0.320 & 7.000 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 47.000 & 0.277 & 29.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 40.667 & 0.241 & 36.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 67.000 & \textbf{0.385} & 3.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 66.667 & 0.377 & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 66.667 & 0.373 & 4.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 61.000 & 0.331 & 10.000 & 0.000 \\
% falcon-mamba-7b-instruct & False & 1 & 53.667 & 0.535 & 45.667 & 0.000 \\
% falcon-mamba-7b-instruct & True & 1 & 56.667 & 0.564 & 44.000 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & False & 1 & 61.667 & 0.613 & 61.000 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & True & 1 & 66.667 & 0.663 & 40.667 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & False & 1 & 63.000 & 0.630 & 51.000 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & True & 1 & 54.667 & 0.453 & 10.000 & 0.000 \\
% falcon-mamba-7b-instruct & False & 2 & 50.667 & 0.506 & 47.333 & 0.000 \\
% falcon-mamba-7b-instruct & True & 2 & 50.667 & 0.393 & 94.667 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & False & 2 & 66.667 & 0.666 & 56.000 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & True & 2 & \textbf{68.333} & \textbf{0.681} & 60.333 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & False & 2 & 59.667 & 0.574 & 74.333 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & True & 2 & 63.667 & 0.634 & 42.333 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ Coarse Grained Aritotle Classes}
\begin{table}[H]
\centering
\caption{Coarse-grained Aritotle Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 35.667 & 0.115 & 3.667 & 2.333 \\
falcon-mamba-7b-instruct & True & 1 & 32.000 & 0.058 & 4.667 & 5.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 50.333 & 0.457 & 0.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 51.333 & 0.156 & 0.000 & 2.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 52.333 & 0.108 & 0.000 & 7.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.333 & 0.107 & 0.000 & 8.667 \\
falcon-mamba-7b-instruct & False & 2 & 37.333 & 0.125 & 0.000 & 9.333 \\
falcon-mamba-7b-instruct & True & 2 & 28.667 & 0.065 & 1.667 & 30.667 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 42.667 & 0.289 & 0.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 53.667 & \textbf{0.292} & 0.667 & 0.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 56.000 & 0.088 & 0.000 & 9.667 \\
Mistral-7B-Instruct-v0.3 & True & 2 & \textbf{59.000} & 0.280 & 0.000 & 1.333 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{graphics/aristotle_confusion_matrix.png}
\caption{\textbf{Best performing model's results Confusion Matrix}}
\end{figure}

\begin{table}[H]
\centering
\caption{Results when mapping fine-grained results to Aritotle's coarse-grained classes}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 40.333 & 0.316 & 16.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 1 & 36.667 & 0.289 & 18.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 63.000 & 0.398 & 1.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 62.667 & 0.408 & 0.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 60.000 & 0.414 & 6.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 57.667 & 0.401 & 7.000 & 0.000 \\
falcon-mamba-7b-instruct & False & 2 & 38.667 & 0.329 & 29.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 38.333 & 0.339 & 36.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{65.333} & \textbf{0.436} & 3.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 65.000 & 0.422 & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 61.000 & 0.359 & 4.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 54.333 & 0.327 & 10.000 & 0.000 \\
% falcon-mamba-7b-instruct & False & 1 & 36.000 & 0.333 & 19.667 & 0.000 \\
% falcon-mamba-7b-instruct & True & 1 & 33.667 & 0.298 & 14.667 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & False & 1 & 50.333 & 0.457 & 13.667 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & True & 1 & 51.667 & 0.469 & 21.333 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & False & 1 & 54.667 & 0.517 & 29.333 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & True & 1 & 51.667 & 0.494 & 24.667 & 0.000 \\
% falcon-mamba-7b-instruct & False & 2 & 38.667 & 0.373 & 32.000 & 0.000 \\
% falcon-mamba-7b-instruct & True & 2 & 33.333 & 0.326 & 39.000 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & False & 2 & 43.333 & 0.397 & 12.333 & 0.000 \\
% Meta-Llama-3.1-8B-Instruct & True & 2 & 54.333 & 0.497 & 14.333 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & False & 2 & 58.000 & 0.558 & 29.000 & 0.000 \\
% Mistral-7B-Instruct-v0.3 & True & 2 & 59.000 & 0.555 & 16.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}



















\section{Basic CoT}

\subsection{ Fine Grained Logic Classes}
\begin{table}[H]
\centering
\caption{Fine-grained Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 3.333 & 0.031 & 66.000 & 1.667 \\
falcon-mamba-7b-instruct & True & 1 & 11.667 & 0.085 & 47.000 & 2.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 37.000 & 0.184 & 0.333 & 0.667 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 35.000 & 0.193 & 0.667 & 1.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 38.333 & 0.146 & 0.000 & 3.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 33.667 & 0.156 & 0.667 & 7.333 \\
falcon-mamba-7b-instruct & False & 2 & 16.667 & 0.134 & 28.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 20.667 & 0.138 & 10.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 35.667 & 0.208 & 1.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & \textbf{39.333} & \textbf{0.246} & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 37.667 & 0.206 & 4.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 33.333 & 0.201 & 4.667 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}



\subsection{ Coarse Grained Copi Classes}
\begin{table}[H]
\centering
\caption{Coarse-grained Copi Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 19.000 & 0.089 & 60.333 & 1.667 \\
falcon-mamba-7b-instruct & True & 1 & 27.000 & 0.093 & 43.333 & 4.000 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 50.667 & 0.223 & 0.667 & 0.333 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 53.000 & 0.229 & 0.667 & 0.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 34.333 & 0.052 & 0.000 & 8.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 49.667 & 0.096 & 0.000 & 2.333 \\
falcon-mamba-7b-instruct & False & 2 & 32.333 & 0.130 & 14.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 41.333 & 0.203 & 3.000 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & \textbf{49.000} & \textbf{0.276} & 0.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 46.333 & 0.269 & 1.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 33.667 & 0.207 & 7.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 47.333 & 0.248 & 6.333 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}



\subsection{ Coarse Grained Aritotle Classes}
\begin{table}[H]
\centering
\caption{Coarse-grained Aritotle Classes Results}
\begin{tabular}{lrrllll}
\toprule
model & def & prompt & accuracy & f1 score & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 11.667 & 0.101 & 72.000 & 0.333 \\
falcon-mamba-7b-instruct & True & 1 & 14.667 & 0.065 & 59.333 & 2.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 52.667 & 0.324 & 1.333 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 55.333 & 0.373 & 1.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 54.000 & 0.121 & 0.000 & 6.000 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 54.667 & 0.131 & 0.000 & 3.667 \\
falcon-mamba-7b-instruct & False & 2 & 30.000 & 0.227 & 17.333 & 0.000 \\
falcon-mamba-7b-instruct & True & 2 & 35.000 & 0.251 & 6.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 38.667 & 0.240 & 0.667 & 0.000 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 48.333 & 0.334 & 2.667 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 2 & \textbf{54.333} & \textbf{0.398} & 6.333 & 0.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 48.667 & 0.352 & 6.667 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}


\section{From Coarse-Grained to Fine-Grained Multi-Round CoT }






\begin{table} [H]
\centering
\caption{COPI Coarse-Grained to Fine-Grained Multi-round CoT}
\begin{tabular}{lrrllllll}
\toprule
model & def & prompt & r1\_accuracy & r1\_f1 & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 39.000 & 0.059 & 4.000 & 0.010 & 0.333 & 63.667 \\
falcon-mamba-7b-instruct & True & 1 & 46.667 & 0.067 & 5.667 & 0.022 & 0.333 & 56.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 53.667 & 0.256 & 29.000 & 0.167 & 0.000 & 0.667 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 53.333 & 0.359 & 28.667 & 0.216 & 0.000 & 0.000 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 40.000 & 0.056 & 31.333 & 0.176 & 0.000 & 18.333 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 49.667 & 0.060 & 29.000 & 0.177 & 0.000 & 1.000 \\
falcon-mamba-7b-instruct & False & 2 & 27.333 & 0.034 & 5.333 & 0.018 & 0.000 & 55.333 \\
falcon-mamba-7b-instruct & True & 2 & 15.000 & 0.024 & 9.667 & 0.049 & 0.000 & 56.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 52.000 & 0.358 & 29.333 & 0.186 & 0.000 & 2.333 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 44.000 & 0.262 & 29.333 & \textbf{0.236} & 0.000 & 8.333 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 31.667 & 0.038 & 29.667 & 0.168 & 0.000 & 21.000 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 45.333 & 0.050 & 36.000 & 0.203 & 0.000 & 2.333 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{ARISTOTLE Coarse-Grained to Fine-Grained Multi-round CoT}
\begin{tabular}{lrrllllll}
\toprule
model & def & prompt & r1\_accuracy & r1\_f1 & acc & f1 & failed & unknown \\
\midrule
falcon-mamba-7b-instruct & False & 1 & 41.000 & 0.103 & 10.000 & 0.040 & 0.333 & 31.667 \\
falcon-mamba-7b-instruct & True & 1 & 36.667 & 0.064 & 13.000 & 0.062 & 0.333 & 17.333 \\
Meta-Llama-3.1-8B-Instruct & False & 1 & 56.000 & 0.465 & 31.000 & 0.182 & 0.000 & 1.333 \\
Meta-Llama-3.1-8B-Instruct & True & 1 & 51.667 & 0.463 & 30.000 & 0.207 & 0.000 & 1.333 \\
Mistral-7B-Instruct-v0.3 & False & 1 & 45.000 & 0.035 & 33.667 & 0.154 & 0.000 & 2.667 \\
Mistral-7B-Instruct-v0.3 & True & 1 & 50.667 & 0.058 & 26.667 & 0.117 & 0.000 & 4.333 \\
falcon-mamba-7b-instruct & False & 2 & 43.333 & 0.108 & 8.667 & 0.037 & 0.000 & 33.333 \\
falcon-mamba-7b-instruct & True & 2 & 36.667 & 0.054 & 12.000 & 0.053 & 0.000 & 19.000 \\
Meta-Llama-3.1-8B-Instruct & False & 2 & 44.667 & 0.316 & 28.667 & 0.172 & 0.000 & 0.667 \\
Meta-Llama-3.1-8B-Instruct & True & 2 & 52.667 & 0.479 & 32.333 & \textbf{0.238} & 0.000 & 0.667 \\
Mistral-7B-Instruct-v0.3 & False & 2 & 45.667 & 0.042 & 39.000 & 0.200 & 0.000 & 2.333 \\
Mistral-7B-Instruct-v0.3 & True & 2 & 51.667 & 0.096 & 29.667 & 0.213 & 0.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}