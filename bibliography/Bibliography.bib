
@misc{jinLogicalFallacyDetection2022,
	title = {Logical {Fallacy} {Detection}},
	url = {http://arxiv.org/abs/2202.13758},
	abstract = {Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46\% on Logic and 4.51\% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Jin, Zhijing and Lalwani, Abhinav and Vaidhya, Tejas and Shen, Xiaoyu and Ding, Yiwen and Lyu, Zhiheng and Sachan, Mrinmaya and Mihalcea, Rada and Schölkopf, Bernhard},
	month = dec,
	year = {2022},
	note = {arXiv:2202.13758 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
	file = {PDF:/home/lefteris/Zotero/storage/PSIDK2XL/Jin et al. - 2022 - Logical Fallacy Detection.pdf:application/pdf},
}

@inproceedings{dasanmartinoFineGrainedAnalysisPropaganda2019,
	address = {Hong Kong, China},
	title = {Fine-{Grained} {Analysis} of {Propaganda} in {News} {Article}},
	url = {https://aclanthology.org/D19-1565},
	doi = {10.18653/v1/D19-1565},
	abstract = {Propaganda aims at influencing people's mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Da San Martino, Giovanni and Yu, Seunghak and Barrón-Cedeño, Alberto and Petrov, Rostislav and Nakov, Preslav},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {5636--5646},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/8N92I74I/Da San Martino et al. - 2019 - Fine-Grained Analysis of Propaganda in News Article.pdf:application/pdf},
}

@article{verspoorLLMsCanCombat,
	title = {{LLMs} can combat {LLM} hallucinations},
	language = {en},
	author = {Verspoor, Karin},
	file = {PDF:/home/lefteris/Zotero/storage/252I8KB6/Verspoor - LLMs can combat LLM hallucinations.pdf:application/pdf},
}

@misc{lucasFightingFireFire2023,
	title = {Fighting {Fire} with {Fire}: {The} {Dual} {Role} of {LLMs} in {Crafting} and {Detecting} {Elusive} {Disinformation}},
	shorttitle = {Fighting {Fire} with {Fire}},
	url = {http://arxiv.org/abs/2310.15515},
	abstract = {Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (i.e., generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel “Fighting Fire with Fire” (F3) strategy that harnesses modern LLMs’ generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLMgenerated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot incontext semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts \& news articles. In our extensive experiments, we observe GPT-3.5-turbo’s zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT3.5-turbo consistently achieved accuracy at 6872\%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Lucas, Jason and Uchendu, Adaku and Yamashita, Michiharu and Lee, Jooyoung and Rohatgi, Shaurya and Lee, Dongwon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15515 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/lefteris/Zotero/storage/LNIRPRYQ/Lucas et al. - 2023 - Fighting Fire with Fire The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation.pdf:application/pdf},
}

@article{farquharDetectingHallucinationsLarge2024,
	title = {Detecting hallucinations in large language models using semantic entropy},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07421-0},
	doi = {10.1038/s41586-024-07421-0},
	abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
	language = {en},
	number = {8017},
	urldate = {2024-09-18},
	journal = {Nature},
	author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information technology, hallucinations},
	pages = {625--630},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/9MCVN9NL/Farquhar et al. - 2024 - Detecting hallucinations in large language models using semantic entropy.pdf:application/pdf},
}

@misc{sprenkampLargeLanguageModels2023,
	title = {Large {Language} {Models} for {Propaganda} {Detection}},
	url = {http://arxiv.org/abs/2310.06422},
	abstract = {The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models’ performance by assessing metrics such as F 1 score, P recision, and Recall, comparing the results with the current state-ofthe-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, this study analyzes the potential and challenges of LLMs in complex tasks like propaganda detection.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Sprenkamp, Kilian and Jones, Daniel Gordon and Zavolokina, Liudmila},
	month = nov,
	year = {2023},
	note = {arXiv:2310.06422 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/home/lefteris/Zotero/storage/PPH4PHUR/Sprenkamp et al. - 2023 - Large Language Models for Propaganda Detection.pdf:application/pdf},
}

@inproceedings{goffredoArgumentbasedDetectionClassification2023,
	address = {Singapore},
	title = {Argument-based {Detection} and {Classification} of {Fallacies} in {Political} {Debates}},
	url = {https://aclanthology.org/2023.emnlp-main.684},
	doi = {10.18653/v1/2023.emnlp-main.684},
	abstract = {Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate conclusions and invalid inferences from the public opinion and the policymakers. Automatically detecting and classifying fallacious arguments represents therefore a crucial challenge to limit the spread of misleading or manipulative claims and promote a more informed and healthier political discourse. Our contribution to address this challenging task is twofold. First, we extend the ElecDeb60To16 dataset of U.S. presidential debates annotated with fallacious arguments, by incorporating the most recent Trump-Biden presidential debate. We include updated token-level annotations, incorporating argumentative components (i.e., claims and premises), the relations between these components (i.e., support and attack), and six categories of fallacious arguments (i.e., Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope, and Slogans). Second, we perform the twofold task of fallacious argument detection and classification by defining neural network architectures based on Transformers models, combining text, argumentative features, and engineered features. Our results show the advantages of complementing transformer-generated text representations with non-text features.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Goffredo, Pierpaolo and Chaves, Mariana and Villata, Serena and Cabrio, Elena},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {11101--11112},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/SLZRSJLC/Goffredo et al. - 2023 - Argument-based Detection and Classification of Fallacies in Political Debates.pdf:application/pdf},
}

@misc{cotterellLowResourceNamedEntity2024,
	title = {Low-{Resource} {Named} {Entity} {Recognition} with {Cross}-{Lingual}, {Character}-{Level} {Neural} {Conditional} {Random} {Fields}},
	url = {http://arxiv.org/abs/2404.09383},
	abstract = {Low-resource named entity recognition is still an open problem in NLP. Most stateof-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world’s languages, it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Cotterell, Ryan and Duh, Kevin},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09383 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/lefteris/Zotero/storage/GJ6CQLDQ/Cotterell and Duh - 2024 - Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random.pdf:application/pdf},
}

@misc{wangGPTNERNamedEntity2023,
	title = {{GPT}-{NER}: {Named} {Entity} {Recognition} via {Large} {Language} {Models}},
	shorttitle = {{GPT}-{NER}},
	url = {http://arxiv.org/abs/2304.10428},
	doi = {10.48550/arXiv.2304.10428},
	abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus\#\# is a city", where special tokens @@\#\# marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
	month = oct,
	year = {2023},
	note = {arXiv:2304.10428 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/KEYE6VRS/Wang et al. - 2023 - GPT-NER Named Entity Recognition via Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/I5XVPMMC/2304.html:text/html},
}

@misc{wangGPTNERNamedEntity2023a,
	title = {{GPT}-{NER}: {Named} {Entity} {Recognition} via {Large} {Language} {Models}},
	shorttitle = {{GPT}-{NER}},
	url = {http://arxiv.org/abs/2304.10428},
	abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
	month = oct,
	year = {2023},
	note = {arXiv:2304.10428 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/lefteris/Zotero/storage/YP3SZ8TQ/Wang et al. - 2023 - GPT-NER Named Entity Recognition via Large Language Models.pdf:application/pdf},
}

@misc{wuRetrievalAugmentedGenerationNatural2024,
	title = {Retrieval-{Augmented} {Generation} for {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2407.13193},
	doi = {10.48550/arXiv.2407.13193},
	abstract = {Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG training, including RAG with/without datastore update. Then, we introduce the application of RAG in representative natural language processing tasks and industrial scenarios. Finally, this paper discusses the future directions and challenges of RAG for promoting its development.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Wu, Shangyu and Xiong, Ying and Cui, Yufei and Wu, Haolun and Chen, Can and Yuan, Ye and Huang, Lianming and Liu, Xue and Kuo, Tei-Wei and Guan, Nan and Xue, Chun Jason},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13193 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/2DX7Q4WH/Wu et al. - 2024 - Retrieval-Augmented Generation for Natural Language Processing A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/GD5J4CFM/2407.html:text/html},
}

@inproceedings{alhindiMultitaskInstructionbasedPrompting2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Multitask {Instruction}-based {Prompting} for {Fallacy} {Recognition}},
	url = {https://aclanthology.org/2022.emnlp-main.560},
	doi = {10.18653/v1/2022.emnlp-main.560},
	abstract = {Fallacies are used as seemingly valid arguments to support a position and persuade the audience about its validity. Recognizing fallacies is an intrinsically difficult task both for humans and machines. Moreover, a big challenge for computational models lies in the fact that fallacies are formulated differently across the datasets with differences in the input format (e.g., question-answer pair, sentence with fallacy fragment), genre (e.g., social media, dialogue, news), as well as types and number of fallacies (from 5 to 18 types per dataset). To move towards solving the fallacy recognition task, we approach these differences across datasets as multiple tasks and show how instruction-based prompting in a multitask setup based on the T5 model improves the results against approaches built for a specific dataset such as T5, BERT or GPT-3. We show the ability of this multitask prompting approach to recognize 28 unique fallacies across domains and genres and study the effect of model size and prompt choice by analyzing the per-class (i.e., fallacy type) results. Finally, we analyze the effect of annotation quality on model performance, and the feasibility of complementing this approach with external knowledge.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alhindi, Tariq and Chakrabarty, Tuhin and Musi, Elena and Muresan, Smaranda},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {8172--8187},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/PL6I3RV2/Alhindi et al. - 2022 - Multitask Instruction-based Prompting for Fallacy Recognition.pdf:application/pdf},
}

@incollection{vorakitphanPROTECTPipelinePropaganda2022,
	title = {{PROTECT} – {A} {Pipeline} for {Propaganda} {Detection} and {Classification}},
	isbn = {9791280136947},
	url = {http://books.openedition.org/aaccademia/10884},
	abstract = {English. Propaganda is a rhetorical technique to present opinions with the deliberate goal of influencing the opinions and the actions of other (groups of) individuals for predetermined misleading ends. The employment of such manipulation techniques in politics and news articles, as well as its subsequent spread on social networks, may lead to threatening consequences for the society and its more vulnerable members. In this paper, we present PROTECT (PROpaganda Text dEteCTion), a new system to automatically detect propagandist messages and classify them along with the propaganda techniques employed. PROTECT is designed as a full pipeline to firstly detect propaganda text snippets from the input text, and then classify the technique of propaganda, taking advantage of semantic and argumentation features. A video demo of the PROTECT system is also provided to show its main functionalities.},
	language = {en},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the {Eighth} {Italian} {Conference} on {Computational} {Linguistics} {CliC}-it 2021},
	publisher = {Accademia University Press},
	author = {Vorakitphan, Vorakit and Cabrio, Elena and Villata, Serena},
	editor = {Fersini, Elisabetta and Passarotti, Marco and Patti, Viviana},
	year = {2022},
	doi = {10.4000/books.aaccademia.10884},
	pages = {352--358},
	file = {PDF:/home/lefteris/Zotero/storage/N22M9ANN/Vorakitphan et al. - 2022 - PROTECT – A Pipeline for Propaganda Detection and Classification.pdf:application/pdf},
}

@inproceedings{sahaiBreakingInvisibleWall2021,
	address = {Online},
	title = {Breaking {Down} the {Invisible} {Wall} of {Informal} {Fallacies} in {Online} {Discussions}},
	url = {https://aclanthology.org/2021.acl-long.53},
	doi = {10.18653/v1/2021.acl-long.53},
	abstract = {People debate on a variety of topics on online platforms such as Reddit, or Facebook. Debates can be lengthy, with users exchanging a wealth of information and opinions. However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. These techniques are called fallacies. Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. In this paper, we study the most frequent fallacies on Reddit, and we present them using the pragma-dialectical theory of argumentation. We construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing. Finally, we study the task of classifying fallacies using neural models. We find that generally the models perform better in the presence of conversational context. We have released the data and the code at github.com/sahaisaumya/informal\_fallacies.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sahai, Saumya and Balalau, Oana and Horincar, Roxana},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {644--657},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/NU7S56Q6/Sahai et al. - 2021 - Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions.pdf:application/pdf},
}

@misc{sprenkampLargeLanguageModels2023a,
	title = {Large {Language} {Models} for {Propaganda} {Detection}},
	url = {http://arxiv.org/abs/2310.06422},
	doi = {10.48550/arXiv.2310.06422},
	abstract = {The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as \$F1\$ score, \$Precision\$, and \$Recall\$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, this study analyzes the potential and challenges of LLMs in complex tasks like propaganda detection.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Sprenkamp, Kilian and Jones, Daniel Gordon and Zavolokina, Liudmila},
	month = nov,
	year = {2023},
	note = {arXiv:2310.06422 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/QQ3JV4ZC/Sprenkamp et al. - 2023 - Large Language Models for Propaganda Detection.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/BVNPCSYM/2310.html:text/html},
}

@misc{limEvaluationLLMIdentifying2024,
	title = {Evaluation of an {LLM} in {Identifying} {Logical} {Fallacies}: {A} {Call} for {Rigor} {When} {Adopting} {LLMs} in {HCI} {Research}},
	shorttitle = {Evaluation of an {LLM} in {Identifying} {Logical} {Fallacies}},
	url = {http://arxiv.org/abs/2404.05213},
	doi = {10.48550/arXiv.2404.05213},
	abstract = {There is increasing interest in the adoption of LLMs in HCI research. However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks. We contend that LLMs should be adopted in a critical manner following rigorous evaluation. Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention. By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90. This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short. The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Lim, Gionnieve and Perrault, Simon T.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.05213 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/RBZKY5AT/Lim and Perrault - 2024 - Evaluation of an LLM in Identifying Logical Fallacies A Call for Rigor When Adopting LLMs in HCI Re.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/FLNNVXQP/2404.html:text/html},
}

@misc{liReasonFallacyEnhancing2024,
	title = {Reason from {Fallacy}: {Enhancing} {Large} {Language} {Models}' {Logical} {Reasoning} through {Logical} {Fallacy} {Understanding}},
	shorttitle = {Reason from {Fallacy}},
	url = {http://arxiv.org/abs/2404.04293},
	doi = {10.48550/arXiv.2404.04293},
	abstract = {Large Language Models (LLMs) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for LLMs' suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate LLMs' capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we have successfully constructed a new dataset LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments justify that our LFUD can be used not only to evaluate LLMs' LFU capability, but also to fine-tune LLMs to obtain significantly enhanced performance on logical reasoning.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Li, Yanda and Wang, Dixuan and Liang, Jiaqing and Jiang, Guochao and He, Qianyu and Xiao, Yanghua and Yang, Deqing},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/5ID3TA87/Li et al. - 2024 - Reason from Fallacy Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Unde.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/XPGFPJP4/2404.html:text/html},
}

@misc{FinetunedLanguageModels,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {https://openreview.net/forum?id=ewdeUNwmJLk},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks...},
	language = {en},
	urldate = {2024-09-24},
	journal = {OpenReview},
	file = {Snapshot:/home/lefteris/Zotero/storage/P4WPACR8/forum.html:text/html},
}

@incollection{vorakitphanPROTECTPipelinePropaganda2022a,
	title = {{PROTECT} – {A} {Pipeline} for {Propaganda} {Detection} and {Classification}},
	isbn = {9791280136947},
	url = {http://books.openedition.org/aaccademia/10884},
	abstract = {English. Propaganda is a rhetorical technique to present opinions with the deliberate goal of influencing the opinions and the actions of other (groups of) individuals for predetermined misleading ends. The employment of such manipulation techniques in politics and news articles, as well as its subsequent spread on social networks, may lead to threatening consequences for the society and its more vulnerable members. In this paper, we present PROTECT (PROpaganda Text dEteCTion), a new system to automatically detect propagandist messages and classify them along with the propaganda techniques employed. PROTECT is designed as a full pipeline to firstly detect propaganda text snippets from the input text, and then classify the technique of propaganda, taking advantage of semantic and argumentation features. A video demo of the PROTECT system is also provided to show its main functionalities.},
	language = {en},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the {Eighth} {Italian} {Conference} on {Computational} {Linguistics} {CliC}-it 2021},
	publisher = {Accademia University Press},
	author = {Vorakitphan, Vorakit and Cabrio, Elena and Villata, Serena},
	editor = {Fersini, Elisabetta and Passarotti, Marco and Patti, Viviana},
	year = {2022},
	doi = {10.4000/books.aaccademia.10884},
	pages = {352--358},
	file = {PDF:/home/lefteris/Zotero/storage/W24IQ6NZ/Vorakitphan et al. - 2022 - PROTECT – A Pipeline for Propaganda Detection and Classification.pdf:application/pdf},
}

@inproceedings{sahaiBreakingInvisibleWall2021a,
	address = {Online},
	title = {Breaking {Down} the {Invisible} {Wall} of {Informal} {Fallacies} in {Online} {Discussions}},
	url = {https://aclanthology.org/2021.acl-long.53},
	doi = {10.18653/v1/2021.acl-long.53},
	abstract = {People debate on a variety of topics on online platforms such as Reddit, or Facebook. Debates can be lengthy, with users exchanging a wealth of information and opinions. However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. These techniques are called fallacies. Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. In this paper, we study the most frequent fallacies on Reddit, and we present them using the pragma-dialectical theory of argumentation. We construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing. Finally, we study the task of classifying fallacies using neural models. We find that generally the models perform better in the presence of conversational context. We have released the data and the code at github.com/sahaisaumya/informal\_fallacies.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sahai, Saumya and Balalau, Oana and Horincar, Roxana},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {644--657},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/8KR5423M/Sahai et al. - 2021 - Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions.pdf:application/pdf},
}

@inproceedings{habernalArgotarioComputationalArgumentation2017,
	address = {Copenhagen, Denmark},
	title = {Argotario: {Computational} {Argumentation} {Meets} {Serious} {Games}},
	shorttitle = {Argotario},
	url = {https://aclanthology.org/D17-2002},
	doi = {10.18653/v1/D17-2002},
	abstract = {An important skill in critical thinking and argumentation is the ability to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to `wrong moves' in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated fallacies empirically. The nonexistence of resources dealing with fallacious argumentation calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a serious game that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net.},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Habernal, Ivan and Hannemann, Raffael and Pollak, Christian and Klamm, Christopher and Pauli, Patrick and Gurevych, Iryna},
	editor = {Specia, Lucia and Post, Matt and Paul, Michael},
	month = sep,
	year = {2017},
	pages = {7--12},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/NJAQ699G/Habernal et al. - 2017 - Argotario Computational Argumentation Meets Serious Games.pdf:application/pdf},
}

@incollection{vorakitphanPROTECTPipelinePropaganda2022b,
	title = {{PROTECT} – {A} {Pipeline} for {Propaganda} {Detection} and {Classification}},
	isbn = {9791280136947},
	url = {http://books.openedition.org/aaccademia/10884},
	abstract = {English. Propaganda is a rhetorical technique to present opinions with the deliberate goal of influencing the opinions and the actions of other (groups of) individuals for predetermined misleading ends. The employment of such manipulation techniques in politics and news articles, as well as its subsequent spread on social networks, may lead to threatening consequences for the society and its more vulnerable members. In this paper, we present PROTECT (PROpaganda Text dEteCTion), a new system to automatically detect propagandist messages and classify them along with the propaganda techniques employed. PROTECT is designed as a full pipeline to firstly detect propaganda text snippets from the input text, and then classify the technique of propaganda, taking advantage of semantic and argumentation features. A video demo of the PROTECT system is also provided to show its main functionalities.},
	language = {en},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the {Eighth} {Italian} {Conference} on {Computational} {Linguistics} {CliC}-it 2021},
	publisher = {Accademia University Press},
	author = {Vorakitphan, Vorakit and Cabrio, Elena and Villata, Serena},
	editor = {Fersini, Elisabetta and Passarotti, Marco and Patti, Viviana},
	year = {2022},
	doi = {10.4000/books.aaccademia.10884},
	pages = {352--358},
	file = {PDF:/home/lefteris/Zotero/storage/Y97JN66T/Vorakitphan et al. - 2022 - PROTECT – A Pipeline for Propaganda Detection and Classification.pdf:application/pdf},
}

@inproceedings{perkovicHallucinationsLLMsUnderstanding2024,
	title = {Hallucinations in {LLMs}: {Understanding} and {Addressing} {Challenges}},
	shorttitle = {Hallucinations in {LLMs}},
	url = {https://ieeexplore.ieee.org/document/10569238/?arnumber=10569238},
	doi = {10.1109/MIPRO60963.2024.10569238},
	abstract = {Large language models (LLM) are trained to understand and generate human-like language. While LLMs present a cutting-edge concept and their use is becoming widespread, hallucinations sometimes occur during their operation. Hallucinations refer to instances where the model generates inaccurate or fictitious information, deviating from factual knowledge and potentially providing responses that lack a basis in model’s training data. In this paper, the ways in which LLMs generate text are examined to address the question of why hallucinations occur. The paper additional explores how existing LLM models can be leveraged to reduce the likelihood of hallucination. Alongside exploring hallucinations, this paper provides insights into the algorithms used for training LLMs, offering a clear picture of the text generation process and its effective utilization.},
	urldate = {2024-09-27},
	booktitle = {2024 47th {MIPRO} {ICT} and {Electronics} {Convention} ({MIPRO})},
	author = {Perković, Gabrijela and Drobnjak, Antun and Botički, Ivica},
	month = may,
	year = {2024},
	note = {ISSN: 2623-8764},
	keywords = {Brain modeling, Entertainment industry, hallucination, Heuristic algorithms, LLM, Technological innovation, Training, Training data, transformer model, Transforms},
	pages = {2084--2088},
	file = {IEEE Xplore Abstract Record:/home/lefteris/Zotero/storage/9AZ9F53D/10569238.html:text/html;IEEE Xplore Full Text PDF:/home/lefteris/Zotero/storage/4W4KDWSS/Perković et al. - 2024 - Hallucinations in LLMs Understanding and Addressing Challenges.pdf:application/pdf},
}

@misc{andriopoulosAugmentingLLMsKnowledge2023,
	title = {Augmenting {LLMs} with {Knowledge}: {A} survey on hallucination prevention},
	shorttitle = {Augmenting {LLMs} with {Knowledge}},
	url = {http://arxiv.org/abs/2309.16459},
	abstract = {Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Andriopoulos, Konstantinos and Pouwelse, Johan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16459 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/home/lefteris/Zotero/storage/U6VLZH9U/Andriopoulos and Pouwelse - 2023 - Augmenting LLMs with Knowledge A survey on hallucination prevention.pdf:application/pdf},
}

@misc{andriopoulosAugmentingLLMsKnowledge2023a,
	title = {Augmenting {LLMs} with {Knowledge}: {A} survey on hallucination prevention},
	shorttitle = {Augmenting {LLMs} with {Knowledge}},
	url = {http://arxiv.org/abs/2309.16459},
	abstract = {Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Andriopoulos, Konstantinos and Pouwelse, Johan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16459 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/home/lefteris/Zotero/storage/64B8J98Y/Andriopoulos and Pouwelse - 2023 - Augmenting LLMs with Knowledge A survey on hallucination prevention.pdf:application/pdf},
}

@misc{MisinformationScience,
	title = {Misinformation in and about science},
	url = {https://www.pnas.org/doi/epdf/10.1073/pnas.1912444117},
	language = {en},
	urldate = {2024-09-28},
	doi = {10.1073/pnas.1912444117},
}

@misc{ScienceFakeNews,
	title = {The science of fake news},
	url = {https://www.science.org/doi/10.1126/science.aao2998},
	language = {en},
	urldate = {2024-09-28},
	doi = {10.1126/science.aao2998},
}

@misc{FallaciesSemifakeNews,
	title = {From fallacies to semi-fake news: {Improving} the identification of misinformation triggers across digital media},
	shorttitle = {From fallacies to semi-fake news},
	url = {https://journals.sagepub.com/doi/epdf/10.1177/09579265221076609},
	language = {en},
	urldate = {2024-09-28},
	doi = {10.1177/09579265221076609},
}

@misc{FallaciesSemifakeNewsa,
	title = {From fallacies to semi-fake news: {Improving} the identification of misinformation triggers across digital media},
	shorttitle = {From fallacies to semi-fake news},
	url = {https://journals.sagepub.com/doi/epdf/10.1177/09579265221076609},
	language = {en},
	urldate = {2024-09-28},
	doi = {10.1177/09579265221076609},
}

@article{jeritPoliticalMisinformation2020,
	title = {Political {Misinformation}},
	volume = {23},
	issn = {1094-2939, 1545-1577},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-polisci-050718-032814},
	doi = {10.1146/annurev-polisci-050718-032814},
	abstract = {Misinformation occurs when people hold incorrect factual beliefs and do so confidently. The problem, first conceptualized by Kuklinski and colleagues in 2000, plagues political systems and is exceedingly difficult to correct. In this review, we assess the empirical literature on political misinformation in the United States and consider what scholars have learned since the publication of that early study. We conclude that research on this topic has developed unevenly. Over time, scholars have elaborated on the psychological origins of political misinformation, and this work has cumulated in a productive way. By contrast, although there is an extensive body of research on how to correct misinformation, this literature is less coherent in its recommendations. Finally, a nascent line of research asks whether people\&apos;s reports of their factual beliefs are genuine or are instead a form of partisan cheerleading. Overall, scholarly research on political misinformation illustrates the many challenges inherent in representative democracy.},
	language = {en},
	number = {Volume 23, 2020},
	urldate = {2024-09-28},
	journal = {Annual Review of Political Science},
	author = {Jerit, Jennifer and Zhao, Yangzi},
	month = may,
	year = {2020},
	note = {Publisher: Annual Reviews},
	pages = {77--94},
	file = {Full Text:/home/lefteris/Zotero/storage/3IBB4U9L/Jerit and Zhao - 2020 - Political Misinformation.pdf:application/pdf},
}

@article{beiseckerShadesFakeNews2024,
	title = {Shades of fake news: how fallacies influence consumers’ perception},
	volume = {33},
	issn = {0960-085X},
	shorttitle = {Shades of fake news},
	url = {https://doi.org/10.1080/0960085X.2022.2110000},
	doi = {10.1080/0960085X.2022.2110000},
	abstract = {So far, fake news has been mostly associated with fabricated content that intends to manipulate or shape opinions. In this manuscript, we aim to establish that the perception of information as fake news is influenced by not only fabricated content but also by the rhetorical device used (i.e., how news authors phrase the message). Based on argumentation theory, we advance that fallacies – a subset of well-known deceptive rhetorical devices – share a conceptual overlap with fake news and are therefore suitable for shedding light on the issue’s grey areas. In a first two-by-two, between-subject, best-worst scaling experiment (case 1), we empirically test whether fallacies are related to the perception of information as fake news and to what extent a reader can identify them. In a second two-by-two experiment, we presume that a reader believes that some of a sender’s messages contain fake news and investigate recipients’ subsequent reactions. We find that users distinguish nuances based on the applied fallacies; however, they will not immediately recognise some fallacies as fake news while overemphasising others. Regarding users’ reactions, we observe a more severe reaction when the message identified as fake news comes from a company instead of an acquaintance.},
	number = {1},
	urldate = {2024-09-28},
	journal = {European Journal of Information Systems},
	author = {Beisecker, Sven and Schlereth, Christian and Hein, Sebastian},
	month = jan,
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0960085X.2022.2110000},
	keywords = {argumentation theory, best-worst scaling, Fake news, fallacies, rhetorical devices, social media},
	pages = {41--60},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/RHBFXDAR/Beisecker et al. - 2024 - Shades of fake news how fallacies influence consumers’ perception.pdf:application/pdf},
}

@misc{zanartuDetectingFallaciesClimate2024,
	title = {Detecting {Fallacies} in {Climate} {Misinformation}: {A} {Technocognitive} {Approach} to {Identifying} {Misleading} {Argumentation}},
	shorttitle = {Detecting {Fallacies} in {Climate} {Misinformation}},
	url = {http://arxiv.org/abs/2405.08254},
	doi = {10.48550/arXiv.2405.08254},
	abstract = {Misinformation about climate change is a complex societal issue requiring holistic, interdisciplinary solutions at the intersection between technology and psychology. One proposed solution is a "technocognitive" approach, involving the synthesis of psychological and computer science research. Psychological research has identified that interventions in response to misinformation require both fact-based (e.g., factual explanations) and technique-based (e.g., explanations of misleading techniques) content. However, little progress has been made on documenting and detecting fallacies in climate misinformation. In this study, we apply a previously developed critical thinking methodology for deconstructing climate misinformation, in order to develop a dataset mapping different types of climate misinformation to reasoning fallacies. This dataset is used to train a model to detect fallacies in climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better than previous works. The fallacies that are easiest to detect include fake experts and anecdotal arguments, while fallacies that require background knowledge, such as oversimplification, misrepresentation, and slothful induction, are relatively more difficult to detect. This research lays the groundwork for development of solutions where automatically detected climate misinformation can be countered with generative technique-based corrections.},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Zanartu, Francisco and Cook, John and Wagner, Markus and Garcia, Julian},
	month = may,
	year = {2024},
	note = {arXiv:2405.08254 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/lefteris/Zotero/storage/DMXV9DHH/Zanartu et al. - 2024 - Detecting Fallacies in Climate Misinformation A Technocognitive Approach to Identifying Misleading.pdf:application/pdf;arXiv.org Snapshot:/home/lefteris/Zotero/storage/CB53JQCS/2405.html:text/html},
}

@article{beiseckerShadesFakeNews2024a,
	title = {Shades of {Fake} {News}: {How} {Fallacies} {Influence} {Consumers}’ {Perception}},
	volume = {33},
	shorttitle = {Shades of {Fake} {News}},
	doi = {10.1080/0960085X.2022.2110000},
	abstract = {So far, fake news has been mostly associated with fabricated content that intends to manipulate or shape opinions. In this manuscript, we aim to establish that the perception of information as fake news is influenced by not only fabricated content but also by the rhetorical device used (i.e., how news authors phrase the message). Based on argumentation theory, we advance that fallacies – a subset of well-known deceptive rhetorical devices – share a conceptual overlap with fake news and are therefore suitable for shedding light on the issue’s grey areas. In a first two-by-two, between-subject, best-worst scaling experiment (case 1), we empirically test whether fallacies are related to the perception of information as fake news and to what extent a reader can identify them. In a second two-by-two experiment, we presume that a reader believes that some of a sender’s messages contain fake news and investigate recipients’ subsequent reactions. We find that users distinguish nuances based on the applied fallacies; however, they will not immediately recognise some fallacies as fake news while overemphasising others. Regarding users’ reactions, we observe a more severe reaction when the message identified as fake news comes from a company instead of an acquaintance.},
	journal = {European Journal of Information Systems},
	author = {Beisecker, Sven and Schlereth, Christian and Hein, Sebastian},
	month = jan,
	year = {2024},
	pages = {41--60},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/TY9T8PFI/Beisecker et al. - 2024 - Shades of Fake News How Fallacies Influence Consumers’ Perception.pdf:application/pdf},
}

@article{beiseckerShadesFakeNews2024b,
	title = {Shades of {Fake} {News}: {How} {Fallacies} {Influence} {Consumers}’ {Perception}},
	volume = {33},
	shorttitle = {Shades of {Fake} {News}},
	doi = {10.1080/0960085X.2022.2110000},
	abstract = {So far, fake news has been mostly associated with fabricated content that intends to manipulate or shape opinions. In this manuscript, we aim to establish that the perception of information as fake news is influenced by not only fabricated content but also by the rhetorical device used (i.e., how news authors phrase the message). Based on argumentation theory, we advance that fallacies – a subset of well-known deceptive rhetorical devices – share a conceptual overlap with fake news and are therefore suitable for shedding light on the issue’s grey areas. In a first two-by-two, between-subject, best-worst scaling experiment (case 1), we empirically test whether fallacies are related to the perception of information as fake news and to what extent a reader can identify them. In a second two-by-two experiment, we presume that a reader believes that some of a sender’s messages contain fake news and investigate recipients’ subsequent reactions. We find that users distinguish nuances based on the applied fallacies; however, they will not immediately recognise some fallacies as fake news while overemphasising others. Regarding users’ reactions, we observe a more severe reaction when the message identified as fake news comes from a company instead of an acquaintance.},
	journal = {European Journal of Information Systems},
	author = {Beisecker, Sven and Schlereth, Christian and Hein, Sebastian},
	month = jan,
	year = {2024},
	pages = {41--60},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/RRW98Y6E/Beisecker et al. - 2024 - Shades of Fake News How Fallacies Influence Consumers’ Perception.pdf:application/pdf},
}

@misc{230915402NavigateEnigmatic,
	title = {[2309.15402] {Navigate} through {Enigmatic} {Labyrinth} {A} {Survey} of {Chain} of {Thought} {Reasoning}: {Advances}, {Frontiers} and {Future}},
	url = {https://arxiv.org/abs/2309.15402},
	urldate = {2024-09-30},
	file = {[2309.15402] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning\: Advances, Frontiers and Future:/home/lefteris/Zotero/storage/A82TITAF/2309.html:text/html},
}

@misc{ChainThoughtPromptingElicits,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
	urldate = {2024-09-30},
	file = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models:/home/lefteris/Zotero/storage/8HSXS9NB/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html:text/html},
}

@misc{wangRATRetrievalAugmented2024,
	title = {{RAT}: {Retrieval} {Augmented} {Thoughts} {Elicit} {Context}-{Aware} {Reasoning} in {Long}-{Horizon} {Generation}},
	shorttitle = {{RAT}},
	url = {http://arxiv.org/abs/2403.05313},
	abstract = {We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63\% on code generation, 16.96\% on mathematical reasoning, 19.2\% on creative writing, and 42.78\% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Wang, Zihao and Liu, Anji and Lin, Haowei and Li, Jiaqi and Ma, Xiaojian and Liang, Yitao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05313 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/home/lefteris/Zotero/storage/KD3R9HQL/Wang et al. - 2024 - RAT Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation.pdf:application/pdf},
}

@article{finocchiaroFallaciesEvaluationReasoning1981,
	title = {Fallacies and the {Evaluation} of {Reasoning}},
	volume = {18},
	issn = {0003-0481},
	url = {https://www.jstor.org/stable/20013887},
	number = {1},
	urldate = {2024-11-05},
	journal = {American Philosophical Quarterly},
	author = {Finocchiaro, Maurice A.},
	year = {1981},
	note = {Publisher: [North American Philosophical Publications, University of Illinois Press]},
	pages = {13--22},
}

@misc{castagnaCanFormalArgumentative2024,
	title = {Can formal argumentative reasoning enhance {LLMs} performances?},
	url = {http://arxiv.org/abs/2405.13036},
	doi = {10.48550/arXiv.2405.13036},
	abstract = {Recent years witnessed significant performance advancements in deep-learning-driven natural language models, with a strong focus on the development and release of Large Language Models (LLMs). These improvements resulted in better quality AI-generated output but rely on resource-expensive training and upgrading of models. Although different studies have proposed a range of techniques to enhance LLMs without retraining, none have considered computational argumentation as an option. This is a missed opportunity since computational argumentation is an intuitive mechanism that formally captures agents' interactions and the information conflict that may arise during such interplays, and so it seems well-suited for boosting the reasoning and conversational abilities of LLMs in a seamless manner. In this paper, we present a pipeline (MQArgEng) and preliminary study to evaluate the effect of introducing computational argumentation semantics on the performance of LLMs. Our experiment's goal was to provide a proof-of-concept and a feasibility analysis in order to foster (or deter) future research towards a fully-fledged argumentation engine plugin for LLMs. Exploratory results using the MT-Bench indicate that MQArgEng provides a moderate performance gain in most of the examined topical categories and, as such, show promise and warrant further research.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Castagna, Federico and Sassoon, Isabel and Parsons, Simon},
	month = may,
	year = {2024},
	note = {arXiv:2405.13036},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/7WFB4X88/Castagna et al. - 2024 - Can formal argumentative reasoning enhance LLMs performances.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/9CVGFKH7/2405.html:text/html},
}

@misc{230809853HowSusceptible,
	title = {[2308.09853] {How} susceptible are {LLMs} to {Logical} {Fallacies}?},
	url = {https://arxiv.org/abs/2308.09853},
	urldate = {2024-11-05},
	file = {[2308.09853] How susceptible are LLMs to Logical Fallacies?:/home/lefteris/Zotero/storage/RWHFB4WU/2308.html:text/html},
}

@misc{helweMAFALDABenchmarkComprehensive2024,
	title = {{MAFALDA}: {A} {Benchmark} and {Comprehensive} {Study} of {Fallacy} {Detection} and {Classification}},
	shorttitle = {{MAFALDA}},
	url = {http://arxiv.org/abs/2311.09761},
	abstract = {We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Helwe, Chadi and Calamai, Tom and Paris, Pierre-Henri and Clavel, Chloé and Suchanek, Fabian},
	month = apr,
	year = {2024},
	note = {arXiv:2311.09761},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/I3EGIQ4T/Helwe et al. - 2024 - MAFALDA A Benchmark and Comprehensive Study of Fallacy Detection and Classification.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/TMDBF8E2/2311.html:text/html},
}

@inproceedings{panAreLLMsGood2024,
	address = {Miami, Florida, USA},
	title = {Are {LLMs} {Good} {Zero}-{Shot} {Fallacy} {Classifiers}?},
	url = {https://aclanthology.org/2024.emnlp-main.794},
	doi = {10.18653/v1/2024.emnlp-main.794},
	abstract = {Fallacies are defective arguments with faulty reasoning. Detecting and classifying them is a crucial NLP task to prevent misinformation, manipulative claims, and biased decisions. However, existing fallacy classifiers are limited by the requirement for sufficient labeled data for training, which hinders their out-of-distribution (OOD) generalization abilities. In this paper, we focus on leveraging Large Language Models (LLMs) for zero-shot fallacy classification. To elicit fallacy-related knowledge and reasoning abilities of LLMs, we propose diverse single-round and multi-round prompting schemes, applying different taskspecific instructions such as extraction, summarization, and Chain-of-Thought reasoning. With comprehensive experiments on benchmark datasets, we suggest that LLMs could be potential zero-shot fallacy classifiers. In general, LLMs under single-round prompting schemes have achieved acceptable zeroshot performances compared to the best fullshot baselines and can outperform them in all OOD inference scenarios and some opendomain tasks. Our novel multi-round prompting schemes can effectively bring about more improvements, especially for small LLMs. Our analysis further underlines the future research on zero-shot fallacy classification. Codes and data are available at: https://github.com/panFJCharlotte98/Fallacy\_Detection.},
	urldate = {2024-11-30},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Fengjun and Wu, Xiaobao and Li, Zongrui and Luu, Anh Tuan},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {14338--14364},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/FBN4UL65/Pan et al. - 2024 - Are LLMs Good Zero-Shot Fallacy Classifiers.pdf:application/pdf},
}

@article{koutsianosChainThoughtPrompting,
	title = {Chain of {Thought} {Prompting} for {Intent} {Classification} using {Large} {Language} {Models}},
	language = {en},
	author = {Koutsianos, Dimitrios},
	file = {PDF:/home/lefteris/Zotero/storage/HPNRXHDW/Koutsianos - Chain of Thought Prompting for Intent Classification using Large Language Models.pdf:application/pdf},
}

@misc{wangSelfConsistencyImprovesChain2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/WBE6MQR9/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/YSYD8ZX3/2203.html:text/html},
}

@misc{zelikmanSTaRBootstrappingReasoning2022,
	title = {{STaR}: {Bootstrapping} {Reasoning} {With} {Reasoning}},
	shorttitle = {{STaR}},
	url = {http://arxiv.org/abs/2203.14465},
	doi = {10.48550/arXiv.2203.14465},
	abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\${\textbackslash}times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
	month = may,
	year = {2022},
	note = {arXiv:2203.14465},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/Y3PGCISL/Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/AMWEMB9F/2203.html:text/html},
}

@misc{kojimaLargeLanguageModels2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/4C9ZHE2E/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/WFMMGHFZ/2205.html:text/html},
}

@misc{zhouLeasttoMostPromptingEnables2023,
	title = {Least-to-{Most} {Prompting} {Enables} {Complex} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10625},
	doi = {10.48550/arXiv.2205.10625},
	abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
	month = apr,
	year = {2023},
	note = {arXiv:2205.10625},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/8YNGE94B/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/V4IRJ24R/2205.html:text/html},
}

@misc{zhouLeasttoMostPromptingEnables2023a,
	title = {Least-to-{Most} {Prompting} {Enables} {Complex} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10625},
	doi = {10.48550/arXiv.2205.10625},
	abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
	month = apr,
	year = {2023},
	note = {arXiv:2205.10625},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/P8FUFMZ3/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/AN36WNQQ/2205.html:text/html},
}

@article{hitchcock2000,
	title = {{\textless}no title{\textgreater}},
	issn = {0144-5340},
	author = {Hitchcock, David},
	month = sep,
	year = {2000},
	note = {Publisher: Taylor \& Francis},
	file = {Full Text PDF:/home/lefteris/Zotero/storage/FB6IUJPK/Hitchcock - 2000 - .pdf:application/pdf},
}

@misc{chuNavigateEnigmaticLabyrinth2024,
	title = {Navigate through {Enigmatic} {Labyrinth} {A} {Survey} of {Chain} of {Thought} {Reasoning}: {Advances}, {Frontiers} and {Future}},
	shorttitle = {Navigate through {Enigmatic} {Labyrinth} {A} {Survey} of {Chain} of {Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2309.15402},
	doi = {10.48550/arXiv.2309.15402},
	abstract = {Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence. Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM's reasoning capabilities, which attracts widespread attention from both academics and industry. In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open questions. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zchuz/CoT-Reasoning-Survey},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
	month = jun,
	year = {2024},
	note = {arXiv:2309.15402},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/X3RLHA4Q/Chu et al. - 2024 - Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning Advances, Frontiers and.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/8YBNP4ZL/2309.html:text/html},
}

@misc{jiMitigatingHallucinationLarge2023,
	title = {Towards {Mitigating} {Hallucination} in {Large} {Language} {Models} via {Self}-{Reflection}},
	url = {http://arxiv.org/abs/2310.06271},
	doi = {10.48550/arXiv.2310.06271},
	abstract = {Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/38HMZ34V/Ji et al. - 2023 - Towards Mitigating Hallucination in Large Language Models via Self-Reflection.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/J8WUWYA2/2310.html:text/html},
}

@misc{dhuliawalaChainofVerificationReducesHallucination2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	doi = {10.48550/arXiv.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/C4N5QNJZ/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Large Language Models.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/DNYVSTGH/2309.html:text/html},
}

@misc{TakeStepBack,
	title = {Take a {Step} {Back}: {Evoking} {Reasoning} via {Abstraction} in {Large} {Language} {Models} {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=3bq3jsvcQ1},
	urldate = {2024-12-10},
	file = {Take a Step Back\: Evoking Reasoning via Abstraction in Large Language Models | OpenReview:/home/lefteris/Zotero/storage/YRMLRX9T/forum.html:text/html},
}

@misc{helweMAFALDABenchmarkComprehensive2024a,
	title = {{MAFALDA}: {A} {Benchmark} and {Comprehensive} {Study} of {Fallacy} {Detection} and {Classification}},
	shorttitle = {{MAFALDA}},
	url = {http://arxiv.org/abs/2311.09761},
	doi = {10.48550/arXiv.2311.09761},
	abstract = {We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.},
	urldate = {2024-12-29},
	publisher = {arXiv},
	author = {Helwe, Chadi and Calamai, Tom and Paris, Pierre-Henri and Clavel, Chloé and Suchanek, Fabian},
	month = apr,
	year = {2024},
	note = {arXiv:2311.09761 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/M8WCR9EV/Helwe et al. - 2024 - MAFALDA A Benchmark and Comprehensive Study of Fallacy Detection and Classification.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/ANEC6Y5I/2311.html:text/html},
}

@misc{huangReasoningLargeLanguage2023,
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Towards {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.10403},
	doi = {10.48550/arXiv.2212.10403},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = may,
	year = {2023},
	note = {arXiv:2212.10403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/AD2XSYIP/Huang and Chang - 2023 - Towards Reasoning in Large Language Models A Survey.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/KPRVAYTN/2212.html:text/html},
}

@misc{zhaoSurveyLargeLanguage2024,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = oct,
	year = {2024},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/lefteris/Zotero/storage/KJI7C7S7/Zhao et al. - 2024 - A Survey of Large Language Models.pdf:application/pdf;Snapshot:/home/lefteris/Zotero/storage/UEACYTAD/2303.html:text/html},
}